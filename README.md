Voici un fichier `README.md` complet pour ce projet, bas√© sur les informations fournies dans le document.

-----

# Benchmark de Performances des Web Services REST

Ce projet a pour objectif d'√©valuer et de comparer les performances de trois stacks techniques Java diff√©rentes pour l'impl√©mentation de services web REST.

[cite\_start]L'√©valuation est r√©alis√©e sur un m√™me domaine m√©tier (Gestion d'Items et Cat√©gories) et une m√™me base de donn√©es (PostgreSQL), en se concentrant sur les impacts des choix d'impl√©mentation[cite: 4].

## üéØ Objectifs

Les m√©triques cl√©s analys√©es pour chaque variante sont :

  * [cite\_start]**Performance API :** Latence (p50/p95/p99), D√©bit (requ√™tes/seconde) et Taux d'erreurs[cite: 5].
  * [cite\_start]**Consommation Ressources :** Empreinte CPU et RAM, activit√© du Garbage Collector (GC) et gestion des threads[cite: 6].
  * [cite\_start]**Co√ªt d'Abstraction :** Comparaison entre un contr√¥leur √©crit manuellement (Variantes A et C) et l'exposition automatique fournie par Spring Data REST (Variante D)[cite: 7].

## üîß Architecture & Variantes

[cite\_start]Le domaine m√©tier est compos√© de deux entit√©s : `Category` (1) et `Item` (N)[cite: 9].

Trois variantes de l'API ont √©t√© impl√©ment√©es pour ce benchmark :

  * [cite\_start]**Variante A :** JAX-RS (Jersey) + JPA/Hibernate[cite: 46].
  * [cite\_start]**Variante C :** Spring Boot + Spring MVC (`@RestController`) + JPA/Hibernate[cite: 47].
  * [cite\_start]**Variante D :** Spring Boot + Spring Data REST (Exposition automatique des repositories)[cite: 48].

## üõ†Ô∏è Stack Technique & Pr√©requis

  * [cite\_start]Java 17 [cite: 80]
  * [cite\_start]PostgreSQL 14+ [cite: 80]
  * Maven (pour le build)
  * Docker & Docker Compose (recommand√© pour la pile de monitoring)
  * [cite\_start]Apache JMeter (pour les tests de charge) [cite: 82]
  * [cite\_start]Prometheus (pour les m√©triques JVM) [cite: 81]
  * [cite\_start]InfluxDB v2 (pour les m√©triques JMeter) [cite: 82]
  * [cite\_start]Grafana (pour la visualisation) [cite: 81]

-----

## üöÄ Installation & Configuration

### 1\. Base de Donn√©es (PostgreSQL)

1.  Assurez-vous qu'une instance PostgreSQL 14+ est en cours d'ex√©cution.
2.  [cite\_start]Cr√©ez les tables `category` et `item` en utilisant les scripts SQL fournis[cite: 11, 22]. [cite\_start]N'oubliez pas les index[cite: 42, 44].
3.  Peuplez la base de donn√©es avec le jeu de donn√©es requis :
      * [cite\_start]**2 000** cat√©gories[cite: 74].
      * [cite\_start]**100 000** items (avec une distribution d'environ 50 items/cat√©gorie)[cite: 75].

### 2\. Pile de Monitoring (Docker)

Le moyen le plus simple de lancer la pile de monitoring est via Docker Compose.

1.  Assurez-vous qu'un fichier `docker-compose.yml` est configur√© pour lancer Prometheus, Grafana et InfluxDB v2.
2.  **InfluxDB :** Apr√®s le lancement, acc√©dez √† l'interface web (ex: `http://localhost:8086`) et cr√©ez :
      * [cite\_start]Une organisation (ex: `perf`)[cite: 117].
      * [cite\_start]Un bucket (ex: `jmeter`)[cite: 117].
      * Un token d'API (API Token) ayant les droits d'√©criture sur le bucket `jmeter`.
3.  **Prometheus :** Configurez `prometheus.yml` pour "scraper" les m√©triques des applications Java (voir ci-dessous).
4.  **Grafana :** Connectez Grafana √† vos deux *Data Sources* (Prometheus et InfluxDB).

### 3\. Lancement des Applications

[cite\_start]Vous devez lancer **une seule variante √† la fois** pour isoler les mesures[cite: 129].

**Variantes C et D (Spring Boot) :**
[cite\_start]Ces variantes utilisent Spring Actuator et Micrometer pour exposer les m√©triques Prometheus[cite: 83].

```bash
# S'assurer que application.properties est configur√© (DB, etc.)
mvn clean package
java -jar target/benchmark-variante-C.jar
```

**Variante A (JAX-RS) :**
[cite\_start]Cette variante n√©cessite l'agent Java JMX Exporter pour exposer ses m√©triques √† Prometheus[cite: 81].

```bash
# T√©l√©charger jmx_prometheus_javaagent.jar et un fichier config.yml
mvn clean package
java -javaagent:/path/to/jmx_prometheus_javaagent.jar=8081:config.yml -jar target/benchmark-variante-A.jar
```

-----

## ‚è±Ô∏è Ex√©cution des Sc√©narios de Charge

Les tests de charge sont d√©finis dans des fichiers `.jmx` (Apache JMeter).

**Configuration JMeter (Bonnes pratiques) :**

  * [cite\_start]**`HTTP Request Defaults` :** √Ä configurer pour pointer vers l'URL de la variante test√©e (ex: `http://localhost:8080`)[cite: 116].
  * [cite\_start]**`CSV Data Set Config` :** Les tests doivent utiliser des fichiers CSV pour les `id` existants (cat√©gories, items) afin de simuler une charge r√©aliste[cite: 115].
  * [cite\_start]**`Backend Listener` :** Doit √™tre configur√© pour envoyer les r√©sultats en temps r√©el vers InfluxDB v2 (en utilisant l'URL, l'organisation, le bucket et le token cr√©√©s √† l'√©tape 2)[cite: 117].
  * [cite\_start]**Listeners Lourds :** Tous les listeners graphiques (comme "View Results Tree") doivent √™tre d√©sactiv√©s pendant les tirs de performance[cite: 118].

### Les 4 Sc√©narios de Test

Ex√©cutez chaque sc√©nario `.jmx` contre la variante d'application en cours d'ex√©cution.

1.  **Sc√©nario 1 : READ-heavy (relation incluse)**

      * [cite\_start]**Mix :** 50% `GET /items`, 20% `GET /items?categoryId=...`, 20% `GET /categories/{id}/items`, 10% `GET /categories` [cite: 89-92].
      * [cite\_start]**Charge :** Paliers de 50, 100 et 200 threads[cite: 93].

2.  **Sc√©nario 2 : JOIN-filter cibl√©**

      * [cite\_start]**Mix :** 70% `GET /items?categoryId=...`, 30% `GET /items/{id}`[cite: 96, 98].
      * [cite\_start]**Charge :** Paliers de 60 et 120 threads[cite: 99].

3.  **Sc√©nario 3 : MIXED (√©critures sur deux entit√©s)**

      * [cite\_start]**Mix :** M√©lange de GET, POST, PUT, DELETE sur les `items` et les `categories` (payloads de \~1 KB) [cite: 101-108].
      * [cite\_start]**Charge :** Paliers de 50 et 100 threads[cite: 109].

4.  **Sc√©nario 4 : HEAVY-body (payload 5 KB)**

      * [cite\_start]**Mix :** 50% `POST /items` (5 KB), 50% `PUT /items/{id}` (5 KB) [cite: 111-112].
      * [cite\_start]**Charge :** Paliers de 30 et 60 threads[cite: 113].

## üìä Analyse des R√©sultats

Pendant que les tests JMeter s'ex√©cutent, observez les dashboards Grafana.

[cite\_start]L'objectif final est de collecter les donn√©es de performance (RPS, p95, Erreurs) et les m√©triques JVM (CPU, Heap, GC, Threads) pour chaque sc√©nario et chaque variante afin de remplir les tableaux d'analyse (T2 √† T7) [cite: 136, 142, 173] [cite\_start]et de formuler des r
[benchmark.pdf](https://github.com/user-attachments/files/23487503/benchmark.pdf)
ecommandations[cite: 174].


